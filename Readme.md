## Social Force Model

The model was adapted and tuned according to the equations and values from this paper: Helbing, D., Farkas, I. J., Molnar, P., & Vicsek, T. (2002). Simulation of pedestrian crowds in normal and evacuation situations. Pedestrian and evacuation dynamics, 21(2), 21-58.
We implemented the sensitivity analysis ourselves.

# Project structure
## Analysis

## Experiments

## Model

## Visualization

# How to install
* Clone this repository using `git clone git@github.com:thijs-blom/agent-based-modelling && cd agent-based-modelling`.
* Create a python virtual environment using `python3 -m venv env`
* Activate the virtual environment using `source ./env/bin/activate` (Linux) or `./env/Scripts/Activate` (Windows).
* From the root directory of the project (containing `requirements.txt`), run `pip install -r requirements.txt`.
Note that it is important to run this command from the correct directory, as `requirements.txt` contains relative paths.

# How to run the code
For the following, we assume any commands are run in the python virtual environment as created in the previous section.
## Running the visualization
The default visualization can be launched by running
```
    $ mesa runserver
```
This runs a simulation as defined in `visualization/server.py`.
To modify the visualised experiment, parameters can be changed in `visualization/server.py`, as documented in `model/socialforce/social_force.py`.
## Running the analysis
### Global sensitivity analysis
The global sensitivity analysis is split into the following three separate parts:
* Generating samples from the parameter space
* Running the experiment for these parameter values
* Analyzing the resulting data

Samples used for analysis can be found in the `samples` directory.
These can be generated by running `python ./generate_samples.py` in the `global_sensitivity_analysis` directory.
This will create distinct samples using Saltelli's sampling scheme with N=512 and 4 parameters, resulting in 512*(4+2) = 3072 distinct sample points.
These are then divided in (roughly) equal partitions to allow running batches of experiments on different computers.
Each partition is replicated 5 times, marked by the final `_x.npy` in the filename.

Using `python ./batch_run.py <filename> <nr_processes>`, the experiments can be run to collect data for the generated samples.
The `<filename>` parameter passed may be any file in the `samples` directory (only the filename, not including the directory).
The script creates a number of processes as specified by the parameter `<nr_processes>`.
This allows multiple experiments to be executed in parallel.
Note that setting this value to a higher value may reduce performance of the computer in other applications during execution.

Analysis is done in the Jupyter notebook `Sobol_Analysis.ipynb` and can be viewed using `jupyter-notebook Sobol_Analysis.ipynb`.

### Local sensitivity analysis
### Validation

## Running the experiments